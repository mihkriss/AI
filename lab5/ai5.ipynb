{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Получение и первичная обработка данных"
      ],
      "metadata": {
        "id": "7zc8sNGGRk3P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YjSPu1uaLRfv"
      },
      "outputs": [],
      "source": [
        "import sqlite3\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GRU, Dense, Activation\n",
        "\n",
        "\n",
        "def get_data():\n",
        "    con = sqlite3.connect(\"wikibooks.sqlite\")\n",
        "    cur = con.cursor()\n",
        "    res = cur.execute(\"SELECT body_text FROM ru\")\n",
        "    all_texts = res.fetchall()\n",
        "    return pd.DataFrame({\"text\": [item[0] for item in all_texts]})\n",
        "\n",
        "def contains_non_russian(text):\n",
        "    russian_alphabet = set('абвгдеёжзийклмнопрстуфхцчшщъыьэюя, .!')\n",
        "    words = text.split()\n",
        "    for word in words:\n",
        "        if any(char.lower() not in russian_alphabet for char in word):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def filter_non_russian_strings(strings):\n",
        "    return [string.lower() for string in strings if not contains_non_russian(string)]\n",
        "\n",
        "# Чтение строк из файла\n",
        "with open(\"dataset.txt\", 'rb') as file:\n",
        "    lines = []\n",
        "    for line in file:\n",
        "        line = line.strip().lower().decode(\"utf-8\", \"ignore\")\n",
        "        if len(line) == 0:\n",
        "            continue\n",
        "        lines.append(line)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RhMQidxj6LV"
      },
      "source": [
        "# **Simple RNN с посимвольной и по-словной токенизацией**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtXHIaqGjzUd"
      },
      "source": [
        "Simple RNN (рекуррентная нейронная сеть) - это тип нейронных сетей, который может обрабатывать последовательности данных, такие как текст, звук или временные ряды. Он имеет обратную связь, позволяющую использовать предыдущие выходные данные для обработки текущего входа.\n",
        "\n",
        "Посимвольная токенизация означает разбиение текста на отдельные символы, а по-словной токенизации - на отдельные слова. Оба подхода могут быть использованы для обработки текста Simple RNN.\n",
        "\n",
        "Давайте рассмотрим примеры кода для посимвольной и по-словной токенизации с использованием библиотеки Python Keras:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4Pm9fLakCCI"
      },
      "source": [
        "### Посимвольная токенизация"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "c7PjEKDlyA9l"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, SimpleRNN, Dense\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Объединение всех строк в одну\n",
        "text = \" \".join(filter_non_russian_strings(lines)[:10000])\n",
        "\n",
        "# Создание множества уникальных символов\n",
        "chars = set([c for c in text])\n",
        "\n",
        "# Количество уникальных символов\n",
        "nb_chars = len(chars)\n",
        "\n",
        "# Создание словарей для преобразования символов в числовые индексы и обратно\n",
        "char2index = {c: i for i, c in enumerate(chars)}\n",
        "index2char = {i: c for i, c in enumerate(chars)}\n",
        "\n",
        "# Длина последовательности символов и шаг для создания подпоследовательностей\n",
        "SEQLEN, STEP = 10, 1\n",
        "input_chars, label_chars = [], []\n",
        "\n",
        "# Создание подпоследовательностей длиной SEQLEN и соответствующих символов-меток\n",
        "for i in range(0, len(text) - SEQLEN, STEP):\n",
        "    input_chars.append(text[i: i + SEQLEN])\n",
        "    label_chars.append(text[i + SEQLEN])\n",
        "\n",
        "# Преобразование последовательностей символов в матрицы one-hot кодирования\n",
        "X = np.zeros((len(input_chars), SEQLEN, nb_chars), dtype=np.float32)\n",
        "y = np.zeros((len(input_chars),), dtype=np.int32)\n",
        "for i, input_char in enumerate(input_chars):\n",
        "    for j, ch in enumerate(input_char):\n",
        "        X[i, j, char2index[ch]] = 1.0\n",
        "    y[i] = char2index[label_chars[i]]\n",
        "\n",
        "# Задание параметров модели и обучения\n",
        "NUM_ITERATIONS = 25\n",
        "NUM_PREDS_PER_EPOCH = 100\n",
        "\n",
        "# Создание модели рекуррентной нейронной сети\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(units=500))\n",
        "model.add(Dense(nb_chars))\n",
        "model.add(Activation(\"softmax\"))\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\") "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X, y, batch_size=128, epochs=5, verbose=1)"
      ],
      "metadata": {
        "id": "jHVZWivoTEXL",
        "outputId": "0ae2fffa-4b6d-44e6-cf59-67ea3c8adc12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "3483/3483 [==============================] - 33s 9ms/step - loss: 1.6798\n",
            "Epoch 2/5\n",
            "3483/3483 [==============================] - 33s 10ms/step - loss: 1.6659\n",
            "Epoch 3/5\n",
            "3483/3483 [==============================] - 34s 10ms/step - loss: 1.6531\n",
            "Epoch 4/5\n",
            "3483/3483 [==============================] - 32s 9ms/step - loss: 1.6459\n",
            "Epoch 5/5\n",
            "3483/3483 [==============================] - 33s 10ms/step - loss: 1.6430\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0c5e6aeec0>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_idx = 0\n",
        "test_chars = input_chars[test_idx]\n",
        "\n",
        "print(\"Генерация из начального символа: %s\" % (test_chars))\n",
        "print(test_chars, end=\"\")\n",
        "for i in range(NUM_PREDS_PER_EPOCH):\n",
        "    X_test = np.zeros((1, SEQLEN, nb_chars))\n",
        "    for j, ch in enumerate(test_chars):\n",
        "        X_test[0, j, char2index[ch]] = 1\n",
        "    pred = model.predict(X_test, verbose=0)[0]\n",
        "    # Следующий символ выбирается с учетом вероятностного распределения\n",
        "    next_index = np.random.choice(nb_chars, p=pred)\n",
        "    next_char = index2char[next_index]\n",
        "    print(next_char, end=\"\")\n",
        "    # Обновление test_chars для следующей итерации\n",
        "    test_chars = test_chars[1:] + next_char\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5q5dKLfPujgn",
        "outputId": "b8f7e33e-0036-4466-e6f2-f460a9302533"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Генерация из начального символа: содержание\n",
            "содержание пунш способрои описан импонтов и метиловностью предычетов при грандкой фина незы в от гоморных виде\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kf-b_BUTkF65"
      },
      "source": [
        "\n",
        "## По-словная токенизация"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92pX2KfQkQzf"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, SimpleRNN, Dense\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Объединение всех строк в одну\n",
        "texts = filter_non_russian_strings(lines)[:1000]\n",
        "\n",
        "# Создание экземпляра класса Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# Обучение токенизатора на массиве строк\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "# Преобразование строк в последовательности числовых индексов\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# Преобразование последовательностей входных и выходных данных\n",
        "input_sequences = []\n",
        "output_sequences = []\n",
        "for seq in sequences:\n",
        "    for i in range(1, len(seq)):\n",
        "        input_sequence = seq[:i]\n",
        "        output_sequence = seq[i]\n",
        "        input_sequences.append(input_sequence)\n",
        "        output_sequences.append(output_sequence)\n",
        "    \n",
        "# Заполнение последовательностей до одной и той же длины\n",
        "input_sequences = list(input_sequences)\n",
        "output_sequences = list(output_sequences)\n",
        "\n",
        "max_sequence_length = max(len(seq) for seq in input_sequences)\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# Преобразование выходных последовательностей в one-hot кодировку\n",
        "output_sequences = to_categorical(output_sequences, num_classes=len(tokenizer.word_index) + 1)\n",
        "\n",
        "\n",
        "# Создание модели\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(tokenizer.word_index) + 1, 100, input_length=max_sequence_length))\n",
        "model.add(SimpleRNN(units=128))\n",
        "model.add(Dense(len(tokenizer.word_index) + 1, activation='softmax'))\n",
        "\n",
        "# Компиляция модели\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Обучение модели\n",
        "model.fit(input_sequences, output_sequences, epochs=15, verbose=1)"
      ],
      "metadata": {
        "id": "sJX9QqhGG4rn",
        "outputId": "09d340fc-0a26-4a7f-bf20-217a9daba77f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "108/108 [==============================] - 22s 183ms/step - loss: 7.5125\n",
            "Epoch 2/15\n",
            "108/108 [==============================] - 12s 111ms/step - loss: 6.9080\n",
            "Epoch 3/15\n",
            "108/108 [==============================] - 10s 95ms/step - loss: 6.5360\n",
            "Epoch 4/15\n",
            "108/108 [==============================] - 9s 85ms/step - loss: 6.1986\n",
            "Epoch 5/15\n",
            "108/108 [==============================] - 7s 67ms/step - loss: 5.7274\n",
            "Epoch 6/15\n",
            "108/108 [==============================] - 8s 73ms/step - loss: 5.2437\n",
            "Epoch 7/15\n",
            "108/108 [==============================] - 7s 63ms/step - loss: 4.7716\n",
            "Epoch 8/15\n",
            "108/108 [==============================] - 8s 74ms/step - loss: 4.3037\n",
            "Epoch 9/15\n",
            "108/108 [==============================] - 8s 78ms/step - loss: 3.8403\n",
            "Epoch 10/15\n",
            "108/108 [==============================] - 7s 66ms/step - loss: 3.3855\n",
            "Epoch 11/15\n",
            "108/108 [==============================] - 8s 74ms/step - loss: 2.9509\n",
            "Epoch 12/15\n",
            "108/108 [==============================] - 7s 66ms/step - loss: 2.5380\n",
            "Epoch 13/15\n",
            "108/108 [==============================] - 7s 68ms/step - loss: 2.1609\n",
            "Epoch 14/15\n",
            "108/108 [==============================] - 8s 72ms/step - loss: 1.8171\n",
            "Epoch 15/15\n",
            "108/108 [==============================] - 6s 60ms/step - loss: 1.5198\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2a17ca9d20>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Предсказание продолжения строки\n",
        "input_text_arr = [\"можно получить\", \"для того чтобы\", \"в россии\", \"если площадь\", \"выполнение\", \"в современном\", \"хронический\", \"коррекция\"]\n",
        "for input_text in input_text_arr:\n",
        "    input_sequence = tokenizer.texts_to_sequences([input_text])\n",
        "    input_sequence = pad_sequences(input_sequence, maxlen=max_sequence_length)\n",
        "    predicted_sequence = model.predict(input_sequence)\n",
        "\n",
        "    # Декодирование предсказанной последовательности в текст\n",
        "    predicted_index = np.argmax(predicted_sequence, axis=-1)[0]\n",
        "    predicted_text = tokenizer.sequences_to_texts([[predicted_index]])\n",
        "    print(f\"Сгенерированный текст: {input_text, predicted_text}\")\n"
      ],
      "metadata": {
        "id": "qZj9qnD3G4JV",
        "outputId": "985055bf-110a-42dd-e527-aeba4b8d5ead",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 133ms/step\n",
            "Сгенерированный текст: ('можно получить', ['свидетельство'])\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Сгенерированный текст: ('для того чтобы', ['развить'])\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Сгенерированный текст: ('в россии', ['не'])\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Сгенерированный текст: ('если площадь', ['параллелограмма'])\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Сгенерированный текст: ('выполнение', ['ягоды'])\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Сгенерированный текст: ('в современном', ['обществе'])\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Сгенерированный текст: ('хронический', ['все'])\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Сгенерированный текст: ('коррекция', ['эстетика'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CzbQsX7keC_"
      },
      "source": [
        "Оба примера создают Simple RNN модель с одним слоем SimpleRNN и выходным слоем Dense. Разница заключается в способе токенизации текста. \n",
        "\n",
        "После токенизации текст преобразуется в последовательность чисел, а затем каждая последовательность дополняется до одной длины, чтобы их можно было использовать в качестве входных данных для Simple RNN.\n",
        "\n",
        "В посимвольном примере используется Tokenizer с параметром char_level=True, чтобы токенизировать текст на отдельные символы. Длина каждой последовательности установлена на maxlen=100. Входной слой SimpleRNN использует эту длину и количество уникальных символов в тексте, чтобы создать соответствующую форму входных данных.\n",
        "\n",
        "В по-словном примере используется стандартный Tokenizer, который токенизирует текст на отдельные слова. Для обработки текста в этом случае используется слой Embedding, который преобразует числа в векторы фиксированной длины. Размер словаря (vocab_size) установлен на количество уникальных слов в тексте + 1 (для учета пустого токена). Длина входных последовательностей также установлена на maxlen=100."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Однонаправленная однослойная и многослойная LSTM c посимвольной токенизацией и токенизацией по словам и на основе BPE**"
      ],
      "metadata": {
        "id": "kTx3_1FM_lea"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выясним наименьшую/наибольшую и среднюю длины строк в нашем массиве предложений"
      ],
      "metadata": {
        "id": "D7TaeXTWBv_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_array = filter_non_russian_strings(lines)\n",
        "# Вычисление максимальной длины строки\n",
        "max_length = max(len(text) for text in text_array)\n",
        "\n",
        "# Вычисление минимальной длины строки\n",
        "min_length = min(len(text) for text in text_array)\n",
        "\n",
        "# Вычисление средней длины строки\n",
        "avg_length = sum(len(text) for text in text_array) / len(text_array)\n",
        "\n",
        "print(\"Максимальная длина строки:\", max_length)\n",
        "print(\"Минимальная длина строки:\", min_length)\n",
        "print(\"Средняя длина строки:\", avg_length)"
      ],
      "metadata": {
        "id": "kneAkUjaBqOg",
        "outputId": "88ee2f63-ddee-4c3b-8dae-78372ff1af98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Максимальная длина строки: 1132\n",
            "Минимальная длина строки: 1\n",
            "Средняя длина строки: 39.692088671954444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Удаляем тексты слишком длинные/слишком короткие"
      ],
      "metadata": {
        "id": "BN1PnY80Bwch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Отфильтруем только строки, которые содержат только русские символы\n",
        "text_array = filter_non_russian_strings(lines)  \n",
        "min_length = 35  # Минимальная длина строки, которую нужно сохранить\n",
        "max_length = 200  # Минимальная длина строки, которую нужно сохранить\n",
        "\n",
        "text_corpus = [text for text in text_array if len(text) >= min_length and len(text) <= max_length]\n",
        "len(text_corpus)"
      ],
      "metadata": {
        "id": "HwKivBk2BvGt",
        "outputId": "165f8c7a-b64d-40f4-9f92-39ae45600ae0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1772"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Однонаправленная однослойная LSTM с посимвольной токенизацией:**"
      ],
      "metadata": {
        "id": "LcEQM_-5_txu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Создаем токенизатор для посимвольной токенизации\n",
        "tokenizer = Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(text_corpus)\n",
        "\n",
        "# Установим значение атрибута num_words\n",
        "tokenizer.num_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Преобразуем текст в числовой формат\n",
        "sequences = tokenizer.texts_to_sequences(text_corpus)\n",
        "\n",
        "# Подготавливаем данные для обучения\n",
        "padded_sequences = pad_sequences(sequences)\n",
        "X = padded_sequences[:, :-1]\n",
        "y = padded_sequences[:, -1]\n",
        "\n",
        "# Создаем модель\n",
        "model = Sequential([\n",
        "    LSTM(64, input_shape=(X.shape[1], 1)),\n",
        "    Dense(tokenizer.num_words, activation='softmax')\n",
        "])\n",
        "\n",
        "# Компилируем модель\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "9EFCr98XBgq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_corpus[:100])"
      ],
      "metadata": {
        "id": "wuBqnRfAQOBL",
        "outputId": "71e6cb5b-6cee-4c6c-e531-91e772df421e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['также можно получить свидетельство за рубежом и для того чтобы использовать его в россии пройти процедуру валидации.', 'пилотские в россии не всегда были бессрочными. раньше свидетельство требовалось продлевать.', 'если площадь параллелограмма равна нулю, то векторы коллинеарны.', 'динамические структуры данныхгигиенические основы и медицинский контроль за физическим воспитанием школьников', 'влейте в шейкер джин, лимонный сок, абсент и яичный белок.', 'дважды процедите коктейль через ситечко в охлажденный коктейльный бокал.', 'дайте пене подняться, а потом аккуратно влейте несколько капель биттера.', 'бокал доверху наполняем кубиками льда.', 'поверх льда наливаем крепкий алкоголь и ананасовый фреш.', 'слегка все размешиваем, после чего добавляем фреш лимона.', 'в качестве украшения используем мякоть свежего ананаса.', 'используя коктейльную ложку, уложи слоями ликер мараскино, лаймовый сок и сверхкрепкий ром', 'молоко налейте в кастрюлю, хорошо подогрейте и снимите с плиты.', 'растворите в нем сахар и шоколадную плитку, предварительно поломав ее на кусочки. все тщательно перемешайте.', 'виски, сок лимона, медовый сироп и пименто драм нужно смешать в шейкере со льдом, затем процедить в рокс с большим кубиком льда и долить вишневый ликер.', 'положи в шейкер малину, ежевику и чернику, добавь черную смородину и подави мадлером', 'налей лимонный сок, сахарный сироп, вишневый сок и джин', 'наполни шейкер кубиками льда и взбей', 'перелей через стрейнер и ситечко в охлажденный коктейльный бокал', 'существует обобщение алгоритма брезенхэма для построения окружностей.', 'для построения окружности по алгоритму брезенхема, а именно координаты', 'установите предварительный диагноз. определите правильные действия в сложившейся ситуации.', 'установите предварительный диагноз. определите правильные действия в сложившейся ситуации.', 'определите план обследования пациентки в условиях стационарного отделения скорой медицинской помощи.', 'определите план обследования пациентки в условиях стационарного отделения скорой медицинской помощи.', 'поставьте диагноз. определите план обследования пациентки в условиях стационарного отделения скорой медицинской помощи.', 'вы можете раскрасить бокал, добавив ломтик ананаса и ломтик лимона.', 'смешайте ингредиенты вместе и залейте на лед.', 'запись и представление звуковых данных', 'наполните бостонский шейкер кубиками льда. д', 'встряхните в шейкере и процедите в бокал для коктейля.', 'в авиаработы входят много видов работ. летное обучение не является авиаработами.', 'работы доступные частным пилотам не все.', 'летные проверки наземных средств радиотехнического обеспечения полетов, авиационной электросвязи и систем светосигнального оборудования аэродромов гражданской авиации.', 'положи в медную кружку мяту, тростниковый сахарный песок и подави мадлером', 'наполни бокал дробленым льдом доверху', 'налей кленовый сироп и шотландский виски', 'добавь персиковый биттер и размешай коктейльной ложкой', 'укрась веточкой мяты и сахарной пудрой', 'наполовину наполните шейкер льдом и добавьте джин и сухой вермут.', 'наливаем в бокал мартини и гарнируем с лимонной закруткой.', 'более одного метода удовлетворяет вашему запросу. вы можете уточнить', 'более одного метода удовлетворяет вашему запросу. вы можете уточнить', 'задайте название и расположение проекта.', 'выберите опции автоматической генерации кода и файлов проекта.', 'при необходимости выберите дополнительные библиотеки. вы не должны выбирать любую из них для обычного использования.', 'наши новые пользователи могут быть сбиты с толку терминологией, но на практике эта терминология достаточно распространена.', 'в высокий бокал положите лед и дольку лимона.', 'все ингредиенты аккуратно перемешайте коктейльной ложкой.', 'наполни хайбол кубиками льда доверху.', 'долей тоник доверху и аккуратно размешай коктейльной ложкой.', 'залить джин, а также лимонный сок и розовую воду.', 'остается только украсить бокал лепестком розы для прекрасной презентации.', 'в бокал со льдом налить джин, затем тоник и положить кружок лимона.', 'налить в хайбол со льдом джин по вкусу, долить тоник.', 'банановый и кунжутный джин с розовой водой и водой из цветков апельсина и тоником', 'имбирное пиво с джином, мятным сиропом, лимонным соком, кофе и ягодами', 'джин с тоником, клубникой и лимоном', 'джин с сахарным сиропом, ябочным и лимонным соком и тоником', 'наполни хайбол кубиками льда доверху', 'добавь пикон, маурин квину и гентиану', 'долей содовую доверху и аккуратно размешай коктейльной ложкой', 'укрась подсушенным кружком апельсина, листиком мяты и вишней', 'правильные ответы выделены жирным шрифтом', 'привлекательные, нейтральные, непривлекательные', 'отличные, хорошие, удовлетворительные, неудовлетворительные', 'ощущение сверхважности своей работы', 'параллельное выполнение нескольких дел', 'негативный психологический климат в коллективе', 'больше концентрации на себе, меньше напряжения', 'больше концентрации на себе, меньше напряжения', 'меньше отвлечений, больше концентрации на деле', 'больше концентрации на себе, меньше напряжения', 'меньше отвлечений, больше концентрации на деле', 'уделяет внимание себе или сохраняет баланс личного и общественного', 'только сохраняет баланс личного и общественного', 'не сохраняет баланс личного и общественного', 'времени на дорогу до работы и обратно', 'учитывать биоритмыпельвиоперитонит, вследствие острого воспаления придатков матки, разрыва тубоовариального абсцесса', 'в связи с тем, что в его сознании переживается настоящее, прошлое и будущее', 'в современном обществе большая часть стрессогенных факторов имеет природу', 'в современном обществе доминирующим типом стресса является', 'в том, что ментальные состояния переживаются телесно и могут вызываться телесными ощущениями', 'зацикливание мышления на негативных мыслях без объективных причин', 'в продуцировании хронического стресса', 'поддержание работоспособности мышц за счёт поддержания перфузии и гликемии', 'это деконструкция стрессогенной ситуации', 'это рефлексия над причинами стресса', 'социальная тревожность индуцирует эндогенную, и наоборот', 'выберите два основных пути модификации стресса', 'коррекция нарушений на фоне развившегося стресса', 'центром интеграции нервных и гуморальных систем', 'для постоянной, длительной работы над собой необходимо такое качество, как', 'к быстрым, короткодействующим конструктивным практикам снижения стресса относятся', 'к неинфекционной патологии относятся', 'к способам модификации метаболических последствий стресса относятся', 'необходимо учитывать биологические, психологические и социальные факторы стресса', 'путём эфферентной и афферентной вегетативной иннервации', 'это работа над собой, сдвиг своей рамки референциальности', 'хронизация стресса с последующей декомпенсацией']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Обучаем модель\n",
        "model.fit(X, y, epochs=10, verbose=1)"
      ],
      "metadata": {
        "id": "lp-zmjCkC-gB",
        "outputId": "09dc32c0-1d3e-4903-e2dc-c4f434516a63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "56/56 [==============================] - 4s 10ms/step - loss: 2.7034 - accuracy: 0.3668\n",
            "Epoch 2/10\n",
            "56/56 [==============================] - 0s 8ms/step - loss: 2.2396 - accuracy: 0.4114\n",
            "Epoch 3/10\n",
            "56/56 [==============================] - 0s 8ms/step - loss: 2.1893 - accuracy: 0.4114\n",
            "Epoch 4/10\n",
            "56/56 [==============================] - 0s 7ms/step - loss: 2.1476 - accuracy: 0.4114\n",
            "Epoch 5/10\n",
            "56/56 [==============================] - 0s 8ms/step - loss: 2.1080 - accuracy: 0.4114\n",
            "Epoch 6/10\n",
            "56/56 [==============================] - 0s 7ms/step - loss: 2.0722 - accuracy: 0.4114\n",
            "Epoch 7/10\n",
            "56/56 [==============================] - 0s 7ms/step - loss: 2.0422 - accuracy: 0.4131\n",
            "Epoch 8/10\n",
            "56/56 [==============================] - 0s 8ms/step - loss: 2.0136 - accuracy: 0.4272\n",
            "Epoch 9/10\n",
            "56/56 [==============================] - 1s 10ms/step - loss: 1.9890 - accuracy: 0.4430\n",
            "Epoch 10/10\n",
            "56/56 [==============================] - 0s 8ms/step - loss: 1.9681 - accuracy: 0.4503\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa1018146a0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Генерируем текст\n",
        "seed_text_arr = [\"можно получить\", \"для того чтобы\", \"в россии\", \"если площадь\", \"выполнение\", \"в современном\", \"хронический\", \"коррекция\"]\n",
        "for input_text in seed_text_arr:\n",
        "    input_sequence = tokenizer.texts_to_sequences(input_text)\n",
        "    input_sequence = pad_sequences(input_sequence, maxlen=model.input_shape[1])\n",
        "    predicted_sequence = model.predict(input_sequence)\n",
        "\n",
        "    # Декодирование предсказанной последовательности в текст\n",
        "    predicted_indices = predicted_sequence.argmax(axis=-1)\n",
        "    predicted_text = ''.join([tokenizer.index_word[idx] for idx in predicted_indices])\n",
        "    generated_text = predicted_text[:10]  # Получаем первые 10 символов\n",
        "\n",
        "    # Соединение исходного и сгенерированного текста\n",
        "    output_text = input_text + generated_text\n",
        "    print(output_text)"
      ],
      "metadata": {
        "id": "-9LjzG9nC8QD",
        "outputId": "3895fa26-3117-46f7-de8d-28209c901430",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 21ms/step\n",
            "можно получить.оу.оя.о..\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "для того чтобы...я.оуоя.\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "в россии.я.о..аа\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "если площадьа..ая..оуа\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "выполнение...о..а.аа\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "в современном.я.о..а.а.\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "хроническийу.о.а.а..а\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "коррекция.о..а.уа.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Однонаправленная однослойная LSTM с токенизацией по словам:**"
      ],
      "metadata": {
        "id": "hKUKl6VvB3uW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Создаем токенизатор для токенизации по словам\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(text_corpus)\n",
        "\n",
        "# Преобразуем текст в числовой формат\n",
        "sequences = tokenizer.texts_to_sequences(text_corpus)\n",
        "\n",
        "# Установим значение атрибута num_words\n",
        "tokenizer.num_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Подготавливаем данные для обучения\n",
        "padded_sequences = pad_sequences(sequences)\n",
        "X = padded_sequences[:, :-1]\n",
        "y = padded_sequences[:, -1]\n",
        "\n",
        "# Создаем модель\n",
        "model = Sequential([\n",
        "    LSTM(256, input_shape=(X.shape[1], 1)),\n",
        "    Dense(tokenizer.num_words, activation='softmax')\n",
        "])\n",
        "\n",
        "# Компилируем модель\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "PmrkMb92B964"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Обучаем модель ~60 эпох\n",
        "model.fit(X, y, epochs=15, verbose=1)"
      ],
      "metadata": {
        "id": "EbyuCAhLDBpQ",
        "outputId": "3f24d86f-f777-402b-8bf2-84f2cd6ff814",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "56/56 [==============================] - 3s 7ms/step - loss: 8.2740 - accuracy: 0.0248\n",
            "Epoch 2/15\n",
            "56/56 [==============================] - 0s 5ms/step - loss: 6.6497 - accuracy: 0.0435\n",
            "Epoch 3/15\n",
            "56/56 [==============================] - 0s 5ms/step - loss: 6.1948 - accuracy: 0.0508\n",
            "Epoch 4/15\n",
            "56/56 [==============================] - 0s 5ms/step - loss: 5.9293 - accuracy: 0.0666\n",
            "Epoch 5/15\n",
            "56/56 [==============================] - 0s 5ms/step - loss: 5.7049 - accuracy: 0.0655\n",
            "Epoch 6/15\n",
            "56/56 [==============================] - 0s 5ms/step - loss: 5.4714 - accuracy: 0.0767\n",
            "Epoch 7/15\n",
            "56/56 [==============================] - 0s 5ms/step - loss: 5.2564 - accuracy: 0.0875\n",
            "Epoch 8/15\n",
            "56/56 [==============================] - 0s 5ms/step - loss: 5.0614 - accuracy: 0.0976\n",
            "Epoch 9/15\n",
            "56/56 [==============================] - 0s 5ms/step - loss: 4.8548 - accuracy: 0.1117\n",
            "Epoch 10/15\n",
            "56/56 [==============================] - 0s 5ms/step - loss: 4.6847 - accuracy: 0.1304\n",
            "Epoch 11/15\n",
            "56/56 [==============================] - 0s 5ms/step - loss: 4.4979 - accuracy: 0.1512\n",
            "Epoch 12/15\n",
            "56/56 [==============================] - 0s 5ms/step - loss: 4.3354 - accuracy: 0.1710\n",
            "Epoch 13/15\n",
            "56/56 [==============================] - 0s 5ms/step - loss: 4.1677 - accuracy: 0.1936\n",
            "Epoch 14/15\n",
            "56/56 [==============================] - 0s 5ms/step - loss: 3.9971 - accuracy: 0.2297\n",
            "Epoch 15/15\n",
            "56/56 [==============================] - 0s 5ms/step - loss: 3.8542 - accuracy: 0.2613\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa100115240>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Предсказание продолжения строки\n",
        "input_text_arr = [\"можно получить\", \"для того чтобы\", \"в россии\", \"если площадь\", \"выполнение\", \"в современном\", \"хронический\", \"коррекция\"]\n",
        "for input_text in input_text_arr:\n",
        "    input_sequence = tokenizer.texts_to_sequences([input_text])\n",
        "    input_sequence = pad_sequences(input_sequence, maxlen=len(padded_sequences[0])-1)\n",
        "    predicted_sequence = model.predict(input_sequence)\n",
        "\n",
        "    # Декодирование предсказанной последовательности в текст\n",
        "    predicted_text = tokenizer.sequences_to_texts([[idx] for idx in predicted_sequence.argmax(axis=-1)])\n",
        "    print(f\"Сгенерированный текст: {input_text, predicted_text[0]}\")"
      ],
      "metadata": {
        "id": "UWl-_WP1DCEn",
        "outputId": "9c9ae85a-2160-4150-fb73-76809cc6241e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 331ms/step\n",
            "Сгенерированный текст: ('можно получить', 'преобразования')\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Сгенерированный текст: ('для того чтобы', 'доверху')\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Сгенерированный текст: ('в россии', 'преобразовании')\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Сгенерированный текст: ('если площадь', 'диагноз')\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Сгенерированный текст: ('выполнение', 'преобразовании')\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Сгенерированный текст: ('в современном', 'исследование')\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Сгенерированный текст: ('хронический', 'ответов')\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Сгенерированный текст: ('коррекция', 'исследование')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Однонаправленная однослойная LSTM с использованием keras_nlp.tokenizers.BytePairTokenizer:**"
      ],
      "metadata": {
        "id": "tXsbosEnCG0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras-nlp"
      ],
      "metadata": {
        "id": "DZStVdAxxiuQ",
        "outputId": "21de1b12-d11c-4086-ce4f-6deed936169e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-nlp\n",
            "  Downloading keras_nlp-0.5.2-py3-none-any.whl (527 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.7/527.7 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras-nlp) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras-nlp) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-nlp) (23.1)\n",
            "Collecting tensorflow-text (from keras-nlp)\n",
            "  Downloading tensorflow_text-2.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text->keras-nlp) (0.13.0)\n",
            "Requirement already satisfied: tensorflow<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text->keras-nlp) (2.12.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (0.4.10)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras-nlp) (3.2.2)\n",
            "Installing collected packages: tensorflow-text, keras-nlp\n",
            "Successfully installed keras-nlp-0.5.2 tensorflow-text-2.12.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "hIAiwv0s04ia",
        "outputId": "a2d8b4dc-af86-47a3-d8ca-473708d37ec5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m102.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.15.1 tokenizers-0.13.3 transformers-4.29.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "# Конвертируем text_corpus в единую строку\n",
        "text = ' '.join(text_corpus)\n",
        "\n",
        "# Создаем токенизатор ByteLevelBPETokenizer\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "tokenizer.train_from_iterator([text])\n",
        "\n",
        "# Конвертируем text_corpus в sequences\n",
        "sequences = [tokenizer.encode(text).ids for text in text_corpus]\n",
        "\n",
        "# Pad sequences\n",
        "padded_sequences = pad_sequences(sequences)\n",
        "\n",
        "# Готовим данные для обучения\n",
        "X = padded_sequences[:, :-1]\n",
        "y = padded_sequences[:, -1]\n",
        "\n",
        "# Создаем модель\n",
        "model = Sequential([\n",
        "    LSTM(64, input_shape=(X.shape[1], 1)),\n",
        "    Dense(tokenizer.get_vocab_size(), activation='softmax')\n",
        "])\n",
        "\n",
        "# Компилируем модель\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "3SFqTEZRCcIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Обучаем модель\n",
        "model.fit(X, y, epochs=10, verbose=1)"
      ],
      "metadata": {
        "id": "l95Iu_wSDHSW",
        "outputId": "6cad2e2d-fc3b-48de-8176-7142e2ab6ea4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "56/56 [==============================] - 2s 6ms/step - loss: 7.9048 - accuracy: 0.3307\n",
            "Epoch 2/10\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 5.6641 - accuracy: 0.4080\n",
            "Epoch 3/10\n",
            "56/56 [==============================] - 0s 8ms/step - loss: 4.4712 - accuracy: 0.4080\n",
            "Epoch 4/10\n",
            "56/56 [==============================] - 0s 8ms/step - loss: 4.2000 - accuracy: 0.4080\n",
            "Epoch 5/10\n",
            "56/56 [==============================] - 0s 7ms/step - loss: 4.0562 - accuracy: 0.4086\n",
            "Epoch 6/10\n",
            "56/56 [==============================] - 0s 7ms/step - loss: 3.9450 - accuracy: 0.4091\n",
            "Epoch 7/10\n",
            "56/56 [==============================] - 0s 8ms/step - loss: 3.8563 - accuracy: 0.4137\n",
            "Epoch 8/10\n",
            "56/56 [==============================] - 0s 7ms/step - loss: 3.7880 - accuracy: 0.4368\n",
            "Epoch 9/10\n",
            "56/56 [==============================] - 0s 9ms/step - loss: 3.7442 - accuracy: 0.4351\n",
            "Epoch 10/10\n",
            "56/56 [==============================] - 0s 8ms/step - loss: 3.7056 - accuracy: 0.4357\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa100115f60>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text_arr = [\"можно получить\", \"для того чтобы\", \"в россии\", \"если площадь\", \"выполнение\", \"в современном\", \"хронический\", \"коррекция\"]\n",
        "for input_text in input_text_arr:\n",
        "    input_sequence = tokenizer.encode_batch([input_text])[0]  # Access the first Encoding object\n",
        "    input_sequence = input_sequence.ids  # Get the token IDs\n",
        "    input_sequence = np.array([input_sequence])  # Convert to a 2D numpy array\n",
        "    input_sequence = pad_sequences(input_sequence, maxlen=model.input_shape[1])\n",
        "    predicted_sequence = model.predict(input_sequence)\n",
        "\n",
        "    # Decode the predicted sequence\n",
        "    predicted_sequence = predicted_sequence.argmax(axis=-1)  # Get the token IDs\n",
        "    predicted_text = tokenizer.decode_batch([predicted_sequence])[0]  # Decode the predicted sequence\n",
        "    print(f\"Сгенерированный текст: {input_text, predicted_text}\")"
      ],
      "metadata": {
        "id": "dEiNdc7bDH0c",
        "outputId": "d053aad4-0cb2-4022-b756-a6a7ca75ab82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "Сгенерированный текст: ('можно получить', '.')\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "Сгенерированный текст: ('для того чтобы', ' ответов')\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "Сгенерированный текст: ('в россии', '.')\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "Сгенерированный текст: ('если площадь', '.')\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "Сгенерированный текст: ('выполнение', ' ответов')\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "Сгенерированный текст: ('в современном', '.')\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "Сгенерированный текст: ('хронический', '.')\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "Сгенерированный текст: ('коррекция', '.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8xuK21JQMG1"
      },
      "source": [
        "# **3. Двунаправленная LSTM**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxZBA6MKhRoI"
      },
      "source": [
        "Выясним наименьшую/наибольшую и среднюю длины строк в нашем массиве предложений"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6tSpughgsE-",
        "outputId": "536dbb65-e78e-4495-dea9-463e176520dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Максимальная длина строки: 1132\n",
            "Минимальная длина строки: 1\n",
            "Средняя длина строки: 39.692088671954444\n"
          ]
        }
      ],
      "source": [
        "text_array = filter_non_russian_strings(lines)\n",
        "# Вычисление максимальной длины строки\n",
        "max_length = max(len(text) for text in text_array)\n",
        "\n",
        "# Вычисление минимальной длины строки\n",
        "min_length = min(len(text) for text in text_array)\n",
        "\n",
        "# Вычисление средней длины строки\n",
        "avg_length = sum(len(text) for text in text_array) / len(text_array)\n",
        "\n",
        "print(\"Максимальная длина строки:\", max_length)\n",
        "print(\"Минимальная длина строки:\", min_length)\n",
        "print(\"Средняя длина строки:\", avg_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i997zyS_nRnd"
      },
      "source": [
        "Удаляем тексты слишком длинные/слишком короткие"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-dYsdSihd62"
      },
      "outputs": [],
      "source": [
        "# Отфильтруем только строки, которые содержат только русские символы\n",
        "text_array = filter_non_russian_strings(lines)  \n",
        "min_length = 35  # Минимальная длина строки, которую нужно сохранить\n",
        "max_length = 200  # Минимальная длина строки, которую нужно сохранить\n",
        "\n",
        "text_corpus = [text for text in text_array if len(text) >= min_length and len(text) <= max_length]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuNT0ToOQYHl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import LSTM, Bidirectional, Dense\n",
        "\n",
        "# Инициализируем токенизатор\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(text_corpus)\n",
        "\n",
        "# Преобразуем текст в числовой формат\n",
        "sequences = tokenizer.texts_to_sequences(text_corpus)\n",
        "\n",
        "# Задаем максимальную длину последовательности\n",
        "max_sequence_length = max([len(seq) for seq in sequences])\n",
        "\n",
        "# Добавляем заполнение для последовательностей разной длины\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# Удаляем последний элемент из padded_sequences и используем его в качестве целевой переменной\n",
        "train_data = padded_sequences[:, :-1]\n",
        "target_data = padded_sequences[:, 1:]\n",
        "\n",
        "# Создаем модель\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_sequence_length-1),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
        "    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(len(tokenizer.word_index) + 1, activation='softmax'))\n",
        "])\n",
        "\n",
        "# Компилируем модель\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QI_RjjKMW_L3",
        "outputId": "63e4c4e0-dbd2-4918-9e5e-d8f7ce54fa42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "56/56 [==============================] - 14s 182ms/step - loss: 5.4729 - accuracy: 0.6884\n",
            "Epoch 2/15\n",
            "56/56 [==============================] - 6s 112ms/step - loss: 2.4102 - accuracy: 0.7196\n",
            "Epoch 3/15\n",
            "56/56 [==============================] - 3s 61ms/step - loss: 2.2530 - accuracy: 0.7237\n",
            "Epoch 4/15\n",
            "56/56 [==============================] - 3s 54ms/step - loss: 2.2274 - accuracy: 0.7272\n",
            "Epoch 5/15\n",
            "56/56 [==============================] - 3s 57ms/step - loss: 2.2132 - accuracy: 0.7276\n",
            "Epoch 6/15\n",
            "56/56 [==============================] - 2s 36ms/step - loss: 2.2012 - accuracy: 0.7276\n",
            "Epoch 7/15\n",
            "56/56 [==============================] - 3s 46ms/step - loss: 2.1908 - accuracy: 0.7276\n",
            "Epoch 8/15\n",
            "56/56 [==============================] - 2s 29ms/step - loss: 2.1804 - accuracy: 0.7277\n",
            "Epoch 9/15\n",
            "56/56 [==============================] - 1s 24ms/step - loss: 2.1680 - accuracy: 0.7279\n",
            "Epoch 10/15\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 2.1531 - accuracy: 0.7286\n",
            "Epoch 11/15\n",
            "56/56 [==============================] - 2s 36ms/step - loss: 2.1317 - accuracy: 0.7288\n",
            "Epoch 12/15\n",
            "56/56 [==============================] - 2s 29ms/step - loss: 2.1020 - accuracy: 0.7301\n",
            "Epoch 13/15\n",
            "56/56 [==============================] - 2s 28ms/step - loss: 2.0694 - accuracy: 0.7325\n",
            "Epoch 14/15\n",
            "56/56 [==============================] - 2s 31ms/step - loss: 2.0363 - accuracy: 0.7346\n",
            "Epoch 15/15\n",
            "56/56 [==============================] - 2s 38ms/step - loss: 2.0002 - accuracy: 0.7370\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa07d0188e0>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "# Обучаем модель\n",
        "model.fit(train_data, target_data, epochs=15, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посмотрим предсказания модели после обучения"
      ],
      "metadata": {
        "id": "u7O8i3g_QvQ_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l260JmkoW_Wr",
        "outputId": "791bc0e8-7a9c-4b44-b252-8eceb6055354",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Сгенерированный текст: ('можно получить', 'и в')\n",
            "Сгенерированный текст: ('для того чтобы', 'в и и и')\n",
            "Сгенерированный текст: ('в россии', 'в и в')\n",
            "Сгенерированный текст: ('если площадь', 'в в')\n",
            "Сгенерированный текст: ('выполнение', 'и')\n",
            "Сгенерированный текст: ('в современном', 'в и и')\n",
            "Сгенерированный текст: ('хронический', '')\n",
            "Сгенерированный текст: ('коррекция', 'в')\n"
          ]
        }
      ],
      "source": [
        "# Генерируем текст\n",
        "seed_text_arr = [\"можно получить\", \"для того чтобы\", \"в россии\", \"если площадь\", \"выполнение\", \"в современном\", \"хронический\", \"коррекция\"]\n",
        "for seed_text in seed_text_arr:\n",
        "    seed_sequence = tokenizer.texts_to_sequences([seed_text])\n",
        "    seed_padded = pad_sequences(seed_sequence, maxlen=max_sequence_length - 1)\n",
        "\n",
        "    generated_sequence = model.predict(seed_padded, verbose=0)\n",
        "    generated_indices = np.argmax(generated_sequence, axis=-1)\n",
        "    generated_text = tokenizer.sequences_to_texts(generated_indices)[0]\n",
        "\n",
        "    print(f\"Сгенерированный текст: {seed_text, generated_text}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}